# üõ†Ô∏è Projeto ETL - Promo√ß√µes Kabum com Selenium + Google Cloud Composer

Este projeto √© uma pipeline de **Extra√ß√£o, Transforma√ß√£o e Carga (ETL)** para coletar dados da se√ß√£o de promo√ß√µes do site **Kabum!**, utilizando **Selenium** para Web Scraping, **BeautifulSoup** para parsing de HTML e o **Google Cloud Composer (Airflow gerenciado)** para orquestra√ß√£o e automa√ß√£o.

## üìÅ Estrutura do Projeto

- `extraction.py`: Respons√°vel por acessar a aba de promo√ß√µes do site Kabum com Selenium, extrair os dados com BeautifulSoup e gerar um arquivo CSV com os produtos promocionais. O CSV √© salvo na camada **Raw** do bucket no GCS.
- `transformation.py`: Realiza a limpeza e transforma√ß√£o dos dados brutos do CSV, aplicando regras espec√≠ficas (como extra√ß√£o de pre√ßos, avalia√ß√µes, descontos, entre outros). O resultado √© um novo CSV que √© salvo na camada **Processed** do bucket no GCS.

## ‚òÅÔ∏è Infraestrutura

- **Orquestra√ß√£o**: [Google Cloud Composer (Airflow)](https://cloud.google.com/composer)
- **Armazenamento**: Google Cloud Storage (GCS)
  - Bucket `kabum-raw`:
    - `raw/promocao/`: Armazena os dados brutos extra√≠dos da Kabum (camada Raw)
  - Bucket `kabum-processed`:
    - `processed/promocao/`: Armazena os dados transformados e limpos (camada Processed)

## üîÑ Fluxo do ETL

1. **Extra√ß√£o** (`extraction.py`):
   - Utiliza **Selenium** para simular a navega√ß√£o no site da Kabum.
   - Com **BeautifulSoup**, extrai as informa√ß√µes dos produtos em promo√ß√£o.
   - Os dados s√£o salvos em um arquivo `.csv` e enviados para a pasta `raw/promocao/` do bucket GCS `kabum-raw`.

2. **Transforma√ß√£o** (`transformation.py`):
   - Os arquivos `.csv` da camada Raw s√£o lidos diretamente do GCS.
   - As transforma√ß√µes aplicadas incluem:
     - Convers√£o de pre√ßos antigos e atuais para float.
     - Extra√ß√£o de percentual de desconto.
     - Extra√ß√£o de avalia√ß√µes e unidades dispon√≠veis.
     - Separa√ß√£o entre nome e detalhes do produto.
     - Padroniza√ß√£o do campo de cr√©dito.
   - Ap√≥s transformado, o novo dataset √© salvo no bucket `kabum-processed`, na pasta `processed/promocao/`.

‚öôÔ∏è Tecnologias Utilizadas

```bash
Python 3.x
Selenium
BeautifulSoup
Pandas
re (express√µes regulares)
Google Cloud Storage
Google Cloud Composer (Apache Airflow)
Google BigQuery
```

## Pr√≥ximos passos
Incluir monitoramento e alertas via Airflow.
Carregar os dados transformados em um Data Warehouse (ex: BigQuery).


## Requisitos
Conta no Google Cloud Platform com o Composer e Storage habilitados.
Service Account com permiss√µes adequadas.
Ambiente com depend√™ncias Python instaladas (requirements.txt).


> Projeto desenvolvido para fins educacionais e de automa√ß√£o de coleta de pre√ßos de produtos em promo√ß√£o no e-commerce Kabum!