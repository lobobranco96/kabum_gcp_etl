# üõ†Ô∏è Projeto ETL - Promo√ß√µes Kabum com Selenium + Google Cloud Composer

Este projeto √© uma pipeline de **Extra√ß√£o, Transforma√ß√£o e Carga (ETL)** para coletar dados da se√ß√£o de promo√ß√µes do site **Kabum!**, utilizando **Selenium** para Web Scraping, **BeautifulSoup** para parsing de HTML e o **Google Cloud Composer (Airflow gerenciado)** para orquestra√ß√£o e automa√ß√£o.

## üìÅ Estrutura do Projeto

- `extraction.py`: Respons√°vel por acessar a aba de promo√ß√µes do site Kabum com Selenium, extrair os dados com BeautifulSoup e gerar um arquivo CSV com os produtos promocionais. O CSV √© salvo na camada **Raw** do bucket no GCS.
- `transformation.py`: Realiza a limpeza e transforma√ß√£o dos dados brutos do CSV, aplicando regras espec√≠ficas (como extra√ß√£o de pre√ßos, avalia√ß√µes, descontos, entre outros). O resultado √© um novo CSV que √© salvo na camada **Processed** do bucket no GCS.

## ‚òÅÔ∏è Infraestrutura

- **Orquestra√ß√£o**: [Google Cloud Composer (Airflow)](https://cloud.google.com/composer)
- **Armazenamento**: Google Cloud Storage (GCS)
  - Bucket `kabum-raw`:
    - `/promocao/`: Armazena os dados brutos extra√≠dos da Kabum (camada Raw)
  - Bucket `kabum-processed`:
    - `/promocao/`: Armazena os dados transformados e limpos (camada Processed)

## üß± Banco de Dados

- **BigQuery**
  - **Dataset:** `kabum_dataset`
  - **Tabela:** `produtos`
  - Utilizado para an√°lise e visualiza√ß√£o dos dados com schemas definidos em JSON.

## üì¶ Cont√™ineres & Imagens

- **Artifact Registry**
  - Reposit√≥rio Docker para armazenar imagens customizadas (ex: Selenium + Chrome).
  - As imagens s√£o versionadas e utilizadas nos jobs de extra√ß√£o via GKE ou Composer.

## ‚ò∏Ô∏è Processamento & Execu√ß√£o

- **Google Kubernetes Engine (GKE)**
  - Cluster `cluster-airflow-leve` criado via Terraform.
  - Usado para execu√ß√£o de workloads pesadas ou paralelas relacionadas ao Airflow.
  - **Configura√ß√£o:**
    - 1 n√≥ `e2-small`
    - Disco de 10 GB
    - Integra√ß√£o com o Composer via permiss√µes IAM e RBAC.

## üîÑ Fluxo do ETL

1. **Extra√ß√£o** (`extraction.py`):
   - Utiliza **Selenium** para simular a navega√ß√£o no site da Kabum.
   - Com **BeautifulSoup**, extrai as informa√ß√µes dos produtos em promo√ß√£o.
   - Os dados s√£o salvos em um arquivo `.csv` e enviados para a pasta `raw/promocao/` do bucket GCS `kabum-raw`.

2. **Transforma√ß√£o** (`transformation.py`):
   - Os arquivos `.csv` da camada Raw s√£o lidos diretamente do GCS.
   - As transforma√ß√µes aplicadas incluem:
     - Convers√£o de pre√ßos antigos e atuais para float.
     - Extra√ß√£o de percentual de desconto.
     - Extra√ß√£o de avalia√ß√µes e unidades dispon√≠veis.
     - Separa√ß√£o entre nome e detalhes do produto.
     - Padroniza√ß√£o do campo de cr√©dito.
   - Ap√≥s transformado, o novo dataset √© salvo no bucket `kabum-processed`, na pasta `/promocao/`.

3. **Carga** (BigQuery):
  - O CSV transformado √© carregado diretamente do bucket kabum-processed para o BigQuery.
  - A tabela de destino √© kabum_dataset.produtos.
  - O schema da tabela √© definido em JSON para garantir a consist√™ncia dos dados.
  - A opera√ß√£o de carga pode ser realizada com o operador GCSToBigQueryOperator no Airflow.

‚öôÔ∏è Tecnologias Utilizadas

```bash
Python 3.x
Selenium
BeautifulSoup
Pandas
re (express√µes regulares)
Google Cloud Storage
Google Cloud Composer (Apache Airflow)
Google BigQuery
```

## Pr√≥ximos passos
Incluir monitoramento e alertas via Airflow.
Carregar os dados transformados em um Data Warehouse (ex: BigQuery).


## Requisitos
Conta no Google Cloud Platform com o Composer e Storage habilitados.
Service Account com permiss√µes adequadas.
Ambiente com depend√™ncias Python instaladas (requirements.txt).


> Projeto desenvolvido para fins educacionais e de automa√ß√£o de coleta de pre√ßos de produtos em promo√ß√£o no e-commerce Kabum!
